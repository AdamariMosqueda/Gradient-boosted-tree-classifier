# Gradient-boosted-tree-classifier

## 1.-Gradient boosting
El algoritmo de Boosting por etapas hacia adelante es tambiÃ©n una estrategia muy codiciosa. En cada paso, el Ã¡rbol que tenemos como soluciÃ³n es el que mÃ¡s reduce Î˜Ì‚ğ‘š =
arg ğ‘šğ‘–ğ‘›Î˜ğ‘š
âˆ‘ ğ¿(ğ‘¦ğ‘–, ğ‘“ğ‘šâˆ’1(ğ‘¥ğ‘–) + ğ‘‡(ğ‘¥ğ‘–, Î˜ğ‘š))ğ‘ğ‘–=1, dado el modelo actual ğ‘“ğ‘šâˆ’1. De esta forma, las predicciones dadas por ğ‘‡(ğ‘¥ğ‘–, Î˜ğ‘š) son anÃ¡logas a las componentes del gradiente con signonegativo ğ‘”ğ‘–ğ‘š = [ğœ•ğ¿(ğ‘¦ğ‘–,ğ‘“(ğ‘¥ğ‘–)ğœ•ğ‘“(ğ‘¥ğ‘–)]ğ‘“(ğ‘¥ğ‘–)=ğ‘“ğ‘šâˆ’1(ğ‘¥ğ‘–) . La principal diferencia entre ambas es que las
componentes procedentes de los Ã¡rboles ğ‘¡ğ‘š = (ğ‘‡(ğ‘¥1,Î˜ğ‘š), ğ‘‡(ğ‘¥2,Î˜ğ‘š), â€¦ , ğ‘‡(ğ‘¥ğ‘,Î˜ğ‘š)) no sonindependientes. EstÃ¡n obligadas a ser las predicciones de un Ã¡rbol de decisiÃ³n de nodo ğ½ğ‘š-
terminal, mientras que el gradiente negativo es la direcciÃ³n de mÃ¡ximo descenso sinrestricciones.

La soluciÃ³n a ğ›¾Ì‚ğ‘—ğ‘š = arg ğ‘šğ‘–ğ‘›ğ›¾ğ‘—ğ‘š âˆ‘ ğ¿(ğ‘¦ğ‘–, ğ‘“ğ‘šâˆ’1(ğ‘¥ğ‘– ğ‘¥) + ğ›¾ğ‘—ğ‘š)ğ‘–âˆˆğ‘…ğ‘—ğ‘š en el mÃ©todo de pasos haciaadelante es anÃ¡loga a la de ğœŒğ‘š = arg ğ‘šğ‘–ğ‘›ğœŒ ğ¿(ğ‘“ğ‘šâˆ’1 âˆ’ ğœŒğ‘”ğ‘š) en el caso del mÃ©todo de mÃ¡ximo descenso, con la diferencia de que en el primero se realiza una bÃºsqueda separada en lÃ­nea paraaquellas componentes de ğ‘¡ğ‘š que corresponden a cada regiÃ³n terminal {ğ‘‡(ğ‘¥ğ‘–, Î˜ğ‘š)}0ğ‘¥ğ‘–âˆˆğ‘…ğ‘—ğ‘š.
Si minimizar la funciÃ³n de pÃ©rdida en el conjunto de datos de entrenamiento ğ¿(ğ‘“) =âˆ‘ ğ¿(ğ‘¦ğ‘–, ğ‘“(ğ‘¥ğ‘–))ğ‘ğ‘–=1 fuese el Ãºnico objetivo, el algoritmo de mÃ¡ximo descenso serÃ­a la mejoropciÃ³n. El gradiente es fÃ¡cil de calcular para cualquier funciÃ³n de perdida diferenciable, mientras que resolver Î˜Ì‚ğ‘š = arg ğ‘šğ‘–ğ‘›Î˜ğ‘šâˆ‘ ğ¿(ğ‘¦ğ‘– , ğ‘“ğ‘šâˆ’1(ğ‘¥ğ‘–) + ğ‘‡(ğ‘¥ğ‘–, Î˜ğ‘š))ğ‘ğ‘–=1 es difÃ­cil debido al ya mencionado criterio de robustez. Desafortunadamente, el gradiente solo estÃ¡ definido para las observaciones del conjunto de entrenamiento, siendo el principal y Ãºltimo objetivo generalizar la expresiÃ³n ğ‘“ğ‘€(ğ‘¥) a los datos del conjunto test.

Una posible soluciÃ³n a este dilema presentado es inducir un Ã¡rbol ğ‘‡(ğ‘¥, Î˜ğ‘š) en la m-Ã©sima iteraciÃ³n cuyas predicciones ğ‘¡ğ‘š estÃ©n tan cerca como sea posible del gradiente negativo. Esto nos conduce a   Ìƒğ‘š = arg ğ‘šğ‘–ğ‘›Î˜ğ‘š âˆ‘(âˆ’ğ‘”ğ‘–ğ‘š âˆ’ ğ‘‡(ğ‘¥ğ‘– , Î˜))2

ExplicÃ¡ndolo con mayor detalle, se trata de ajustar un Ã¡rbol ğ‘‡ a los valores del gradiente con signo negativo mediante mÃ­nimos cuadrados. Como vimos anteriormente, existen algoritmos rÃ¡pidos para inducir Ã¡rboles de decisiÃ³n mediante mÃ­nimos cuadrados. Aunque las regiones ğ‘…Ìƒ ğ‘—ğ‘šque obtenemos mediante la soluciÃ³n a Î˜Ìƒğ‘š = arg ğ‘šğ‘–ğ‘›Î˜ğ‘š
âˆ‘ (âˆ’ğ‘”ğ‘–ğ‘š âˆ’ ğ‘‡(ğ‘¥ğ‘– , Î˜)) ğ‘ 2 ğ‘–=1 no son idÃ©nticas a las obtenidas ğ‘…ğ‘—ğ‘š resolviendo Î˜Ì‚ğ‘š = arg ğ‘šğ‘–ğ‘›Î˜ğ‘š âˆ‘ ğ¿(ğ‘¦ğ‘– , ğ‘“ğ‘šâˆ’1 (ğ‘¥ğ‘– ) + ğ‘‡(ğ‘¥ğ‘– , Î˜ğ‘š)) ğ‘ ğ‘–=1 , sÃ­ podemos decir que son similares, ya que persiguen un mismo objetivo. En cualquier caso, el procedimiento de Boosting mediante pasos hacia adelante y la inducciÃ³n de un Ã¡rbol de decisiÃ³n
de manera descendente son procedimientos de aproximaciÃ³n. Tras construir el Ã¡rbol mediante la expresiÃ³n Î˜Ìƒğ‘š = arg ğ‘šğ‘–ğ‘›Î˜ğ‘š âˆ‘ (âˆ’ğ‘”ğ‘–ğ‘š âˆ’ ğ‘‡(ğ‘¥ğ‘– , Î˜)) ğ‘ 2 ğ‘–=1 , el valor que toman las observaciones que caen en una regiÃ³n viene dado por la expresiÃ³n 
