# Gradient Boosted tree classifier 

## Components
Gradient Boosting has three main components:

1. Loss Function - The role of the loss function is to estimate how good the model is at making predictions with the given data. This could vary depending on the problem at hand. For example, if we're trying to predict the weight of a person depending on some input variables (a regression problem), then the loss function would be something that helps us find the difference between the predicted weights and the observed weights. On the other hand, if we're trying to categorize if a person will like a certain movie based on their personality, we'll require a 
loss function that helps us understand how accurate our model is at classifying people who did or didn't like certain movies.

2. Weak Learner - A weak learner is one that classifies our data but does so poorly, perhaps no better than random guessing. In other words, it has a high error rate. These are typically decision trees (also called decision stumps, because they are less complicated than typical decision trees).

1. Additive Model - This is the iterative and sequential approach of adding the trees (weak learners) one step at a time. After each iteration, we need to be closer to our final model. In other words, each iteration should reduce the value of our loss function.



## Gradient boosting vs Adaptive boosting

| Gradient boosting | Adaptive boosting | 
| --- | --- |
| This approach trains learners based upon minimising the loss function of a learner (i.e., training on the residuals of the model)  | This method focuses on training upon misclassified observations. Alters the distribution of the training dataset to increase weights on sample observations that are difficult to classify.| 
| Weak learners are decision trees constructed in a greedy manner with split points based on purity scores (i.e., Gini, minimise loss). Thus, larger trees can be used with around 4 to 8 levels. Learners should still remain weak and so they should be constrained (i.e., the maximum number of layers, nodes, splits, leaf nodes) | The weak learners incase of adaptive boosting are a very basic form of decision tree known as stumps. |
| All the learners have equal weights in the case of gradient boosting. The weight is usually set as the learning rate which is small in magnitude. | The final prediction is based on a majority vote of the weak learnersâ€™ predictions weighted by their individual accuracy. | 


# Gradient-boosted-tree-classifier

## 1.-Gradient boosting
El algoritmo de Boosting por etapas hacia adelante es tambiÃ©n una estrategia muy codiciosa. En cada paso, el Ã¡rbol que tenemos como soluciÃ³n es el que mÃ¡s reduce Î˜Ì‚ğ‘š =
arg ğ‘šğ‘–ğ‘›Î˜ğ‘š
âˆ‘ ğ¿(ğ‘¦ğ‘–, ğ‘“ğ‘šâˆ’1(ğ‘¥ğ‘–) + ğ‘‡(ğ‘¥ğ‘–, Î˜ğ‘š))ğ‘ğ‘–=1, dado el modelo actual ğ‘“ğ‘šâˆ’1. De esta forma, las predicciones dadas por ğ‘‡(ğ‘¥ğ‘–, Î˜ğ‘š) son anÃ¡logas a las componentes del gradiente con signonegativo ğ‘”ğ‘–ğ‘š = [ğœ•ğ¿(ğ‘¦ğ‘–,ğ‘“(ğ‘¥ğ‘–)ğœ•ğ‘“(ğ‘¥ğ‘–)]ğ‘“(ğ‘¥ğ‘–)=ğ‘“ğ‘šâˆ’1(ğ‘¥ğ‘–) . La principal diferencia entre ambas es que las
componentes procedentes de los Ã¡rboles ğ‘¡ğ‘š = (ğ‘‡(ğ‘¥1,Î˜ğ‘š), ğ‘‡(ğ‘¥2,Î˜ğ‘š), â€¦ , ğ‘‡(ğ‘¥ğ‘,Î˜ğ‘š)) no sonindependientes. EstÃ¡n obligadas a ser las predicciones de un Ã¡rbol de decisiÃ³n de nodo ğ½ğ‘š-
terminal, mientras que el gradiente negativo es la direcciÃ³n de mÃ¡ximo descenso sinrestricciones.

La soluciÃ³n a ğ›¾Ì‚ğ‘—ğ‘š = arg ğ‘šğ‘–ğ‘›ğ›¾ğ‘—ğ‘š âˆ‘ ğ¿(ğ‘¦ğ‘–, ğ‘“ğ‘šâˆ’1(ğ‘¥ğ‘– ğ‘¥) + ğ›¾ğ‘—ğ‘š)ğ‘–âˆˆğ‘…ğ‘—ğ‘š en el mÃ©todo de pasos haciaadelante es anÃ¡loga a la de ğœŒğ‘š = arg ğ‘šğ‘–ğ‘›ğœŒ ğ¿(ğ‘“ğ‘šâˆ’1 âˆ’ ğœŒğ‘”ğ‘š) en el caso del mÃ©todo de mÃ¡ximo descenso, con la diferencia de que en el primero se realiza una bÃºsqueda separada en lÃ­nea paraaquellas componentes de ğ‘¡ğ‘š que corresponden a cada regiÃ³n terminal {ğ‘‡(ğ‘¥ğ‘–, Î˜ğ‘š)}0ğ‘¥ğ‘–âˆˆğ‘…ğ‘—ğ‘š.
Si minimizar la funciÃ³n de pÃ©rdida en el conjunto de datos de entrenamiento ğ¿(ğ‘“) =âˆ‘ ğ¿(ğ‘¦ğ‘–, ğ‘“(ğ‘¥ğ‘–))ğ‘ğ‘–=1 fuese el Ãºnico objetivo, el algoritmo de mÃ¡ximo descenso serÃ­a la mejoropciÃ³n. El gradiente es fÃ¡cil de calcular para cualquier funciÃ³n de perdida diferenciable, mientras que resolver Î˜Ì‚ğ‘š = arg ğ‘šğ‘–ğ‘›Î˜ğ‘šâˆ‘ ğ¿(ğ‘¦ğ‘– , ğ‘“ğ‘šâˆ’1(ğ‘¥ğ‘–) + ğ‘‡(ğ‘¥ğ‘–, Î˜ğ‘š))ğ‘ğ‘–=1 es difÃ­cil debido al ya mencionado criterio de robustez. Desafortunadamente, el gradiente solo estÃ¡ definido para las observaciones del conjunto de entrenamiento, siendo el principal y Ãºltimo objetivo generalizar la expresiÃ³n ğ‘“ğ‘€(ğ‘¥) a los datos del conjunto test.

Una posible soluciÃ³n a este dilema presentado es inducir un Ã¡rbol ğ‘‡(ğ‘¥, Î˜ğ‘š) en la m-Ã©sima iteraciÃ³n cuyas predicciones ğ‘¡ğ‘š estÃ©n tan cerca como sea posible del gradiente negativo. Esto nos conduce a   Ìƒğ‘š = arg ğ‘šğ‘–ğ‘›Î˜ğ‘š âˆ‘(âˆ’ğ‘”ğ‘–ğ‘š âˆ’ ğ‘‡(ğ‘¥ğ‘– , Î˜))2

ExplicÃ¡ndolo con mayor detalle, se trata de ajustar un Ã¡rbol ğ‘‡ a los valores del gradiente con signo negativo mediante mÃ­nimos cuadrados. Como vimos anteriormente, existen algoritmos rÃ¡pidos para inducir Ã¡rboles de decisiÃ³n mediante mÃ­nimos cuadrados. Aunque las regiones ğ‘…Ìƒ ğ‘—ğ‘šque obtenemos mediante la soluciÃ³n a Î˜Ìƒğ‘š = arg ğ‘šğ‘–ğ‘›Î˜ğ‘š
âˆ‘ (âˆ’ğ‘”ğ‘–ğ‘š âˆ’ ğ‘‡(ğ‘¥ğ‘– , Î˜)) ğ‘ 2 ğ‘–=1 no son idÃ©nticas a las obtenidas ğ‘…ğ‘—ğ‘š resolviendo Î˜Ì‚ğ‘š = arg ğ‘šğ‘–ğ‘›Î˜ğ‘š âˆ‘ ğ¿(ğ‘¦ğ‘– , ğ‘“ğ‘šâˆ’1 (ğ‘¥ğ‘– ) + ğ‘‡(ğ‘¥ğ‘– , Î˜ğ‘š)) ğ‘ ğ‘–=1 , sÃ­ podemos decir que son similares, ya que persiguen un mismo objetivo. En cualquier caso, el procedimiento de Boosting mediante pasos hacia adelante y la inducciÃ³n de un Ã¡rbol de decisiÃ³n
de manera descendente son procedimientos de aproximaciÃ³n. Tras construir el Ã¡rbol mediante la expresiÃ³n Î˜Ìƒğ‘š = arg ğ‘šğ‘–ğ‘›Î˜ğ‘š âˆ‘ (âˆ’ğ‘”ğ‘–ğ‘š âˆ’ ğ‘‡(ğ‘¥ğ‘– , Î˜)) ğ‘ 2 ğ‘–=1 , el valor que toman las observaciones que caen en una regiÃ³n viene dado por la expresiÃ³n 
